{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "root_dir = '.'\n",
    "tasks = ['atis', 'snips', 'clinic150', 'massive']\n",
    "encoder = 'bge-large-en-v1.5'\n",
    "config = 'e+p+om'\n",
    "top_ks = [3, 16, 151]\n",
    "\n",
    "models = [\n",
    "    'Mistral-7B-Instruct-v0.3',\n",
    "    'llama/Meta-Llama-3.1-8B-Instruct',\n",
    "    'gemma-2-9b-it',\n",
    "    'Phi-3-medium-4k-instruct'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def format_prompt(candidates: List[str], descriptions: Dict[str, str], utterance: str):\n",
    "    \"\"\" Given the similarity scores for candidate classes, format prompt to\n",
    "        pass to LLM for candidate selection\n",
    "\n",
    "    Args:\n",
    "        candidates (List[str]): list of candidates intents, index\n",
    "                                corresponds to sims index\n",
    "        descriptions (Dict[str, str]): textual descriptions corresponding to\n",
    "                                        each intent class\n",
    "        utterance (str): original utterance for model to evaluate\n",
    "    \"\"\"\n",
    "    output_text = '\\n'.join([\n",
    "        f\"Given the user said \\\"{utterance}\\\"\\nPlease give the 'intent' that best reflect what the user is saying/asking for, based on which of the following intents has a description best matching the user's utterance:\",\n",
    "        \"\",\n",
    "        *[f\"intent: {intent}\\ndescription: {descriptions[intent]}\\n\" for intent in candidates],\n",
    "        \"\",\n",
    "        \"Please give the intent name only, do not provide reasoning.\",\n",
    "        \"The intent is: \"\n",
    "    ])\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:41<00:00,  8.46s/it]\n",
      "100%|██████████| 12/12 [01:40<00:00,  8.39s/it]\n",
      "100%|██████████| 12/12 [01:54<00:00,  9.51s/it]\n",
      "100%|██████████| 12/12 [01:48<00:00,  9.06s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for model in models:\n",
    "    model_path = f\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    combinations = list(itertools.product(top_ks, tasks))\n",
    "    for (top_k, task) in tqdm(combinations):\n",
    "        output_tokens_count = []\n",
    "        all_candidates = json.load(open(f\"{root_dir}/candidates/{task}-{encoder}-{config}-{top_k}-cands.jsonl\"))\n",
    "        descriptions = json.load(open(f\"{root_dir}/data/{task}/descriptions.json\"))\n",
    "        data = json.load(open(f\"{root_dir}/data/{task}/data-full-shuffled.json\"))['data']\n",
    "        for (entry, candidates) in zip(data, all_candidates):\n",
    "            prompt = format_prompt(candidates, descriptions, entry['text'])\n",
    "            tokens = tokenizer(prompt).input_ids\n",
    "            output_tokens_count.append(len(tokens))\n",
    "        output_tokens_count = np.array(output_tokens_count)\n",
    "        outputs.append({\n",
    "            'model': model,\n",
    "            'top_k': top_k,\n",
    "            'task': task,\n",
    "            'mean_tok': output_tokens_count.mean()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(outputs)\n",
    "output_df2 = []\n",
    "\n",
    "for model in models:\n",
    "    for top_k in top_ks:\n",
    "        entry = {\n",
    "            'model': model,\n",
    "            'top_k': top_k\n",
    "        }\n",
    "        for task in tasks:\n",
    "            entry[task] = output_df[((output_df['model'] == model)\n",
    "                                     & (output_df['top_k'] == top_k)\n",
    "                                     & (output_df['task'] == task))]['mean_tok'].item()\n",
    "        output_df2.append(entry)\n",
    "output_df2 = pd.DataFrame(output_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df2.to_excel(f\"{root_dir}/results/analysis/input_token_count.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
