{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate effect of quantization on model predictions and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import MODEL_TO_NAME, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = ''\n",
    "quantization = [\n",
    "    '4_bit_quant',\n",
    "    '8_bit_quant',\n",
    "    '16_bit_quant',\n",
    "    'full'\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    'atis',\n",
    "    'snips',\n",
    "    'clinic150',\n",
    "    'massive'\n",
    "]\n",
    "\n",
    "models = [\n",
    "    'llama-3.1-8b-instruct',\n",
    "    'gemma-2-9b-it',\n",
    "    'phi-3-medium-4k-instruct',\n",
    "    'mistral-7b-instruct'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tallying model llama-3.1-8b-instruct\n",
      "4_bit_quant failed mapping rate: 0.0056\n",
      "8_bit_quant failed mapping rate: 0.0055\n",
      "full failed mapping rate: 0.0047\n",
      "\n",
      "Tallying model gemma-2-9b-it\n",
      "4_bit_quant failed mapping rate: 0.0145\n",
      "8_bit_quant failed mapping rate: 0.0123\n",
      "full failed mapping rate: 0.0118\n",
      "\n",
      "Tallying model phi-3-medium-4k-instruct\n",
      "4_bit_quant failed mapping rate: 0.1016\n",
      "8_bit_quant failed mapping rate: 0.0077\n",
      "16_bit_quant failed mapping rate: 0.0076\n",
      "\n",
      "Tallying model mistral-7b-instruct\n",
      "4_bit_quant failed mapping rate: 0.0519\n",
      "8_bit_quant failed mapping rate: 0.0608\n",
      "full failed mapping rate: 0.0607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overall_performance = []\n",
    "overall_correlation = []\n",
    "for model_type in models:\n",
    "    print(f\"Tallying model {model_type}\")\n",
    "    model_name = MODEL_TO_NAME[model_type]\n",
    "    model_entry = []\n",
    "    for precision in quantization:\n",
    "        precision_preds = []\n",
    "        if os.path.exists(f\"{root_dir}/results/{model_name}/{precision}\"):\n",
    "            precision_entry = {\n",
    "                'model': model_type,\n",
    "                'quantization': precision,\n",
    "            }\n",
    "\n",
    "            for task in tasks:\n",
    "                _, preds, texts, model_outs, _ = load_data(task, model_type, model_name, precision)\n",
    "                precision_preds.append(preds)  # failed preds are -1\n",
    "                precision_entry[f\"{task}-preds\"] = preds\n",
    "                precision_entry[f\"{task}-fr\"] = (preds == -1).float().mean()\n",
    "\n",
    "            precision_preds = torch.concat(precision_preds)\n",
    "            failed_map_rate = (precision_preds == -1).float().mean()\n",
    "            print(f\"{precision} failed mapping rate: {failed_map_rate:.4f}\")\n",
    "            precision_entry['failed_map_rate'] = failed_map_rate\n",
    "            precision_entry['all_preds'] = precision_preds\n",
    "\n",
    "            model_entry.append(precision_entry)\n",
    "    model_entry = pd.DataFrame(model_entry)\n",
    "\n",
    "    model_quants = model_entry['quantization'].to_list()\n",
    "    for i in range(len(model_quants)):\n",
    "        for j in range(i + 1, len(model_quants)):\n",
    "            precision_a = model_quants[i]\n",
    "            precision_b = model_quants[j]\n",
    "            correlation_entry = {\n",
    "                'model': model_type,\n",
    "                'precision_a': precision_a,\n",
    "                'precision_b': precision_b,\n",
    "            }\n",
    "\n",
    "            model_entry_a = model_entry[(model_entry['model'] == model_type) & (model_entry['quantization'] == precision_a)]\n",
    "            model_entry_b = model_entry[(model_entry['model'] == model_type) & (model_entry['quantization'] == precision_b)]\n",
    "\n",
    "            for task in tasks:\n",
    "                preds_a = model_entry_a[f\"{task}-preds\"].item()\n",
    "                preds_b = model_entry_b[f\"{task}-preds\"].item()\n",
    "                task_corr_ab = np.corrcoef(preds_a, preds_b)[0, 1]\n",
    "                correlation_entry[f\"{task}_fr_a\"] = (preds_a == -1).float().mean().item()\n",
    "                correlation_entry[f\"{task}_fr_b\"] = (preds_b == -1).float().mean().item()\n",
    "                correlation_entry[f\"{task}-corr-ab\"] = task_corr_ab\n",
    "\n",
    "            correlation_entry['failed_map_rate_a'] = model_entry_a['failed_map_rate'].item().item()\n",
    "            correlation_entry['failed_map_rate_b'] = model_entry_b['failed_map_rate'].item().item()\n",
    "            preds_a = model_entry_a['all_preds'].item()\n",
    "            preds_b = model_entry_b['all_preds'].item()\n",
    "\n",
    "            non_failed_preds_idx = (preds_a != -1) & (preds_b != -1)\n",
    "            ovr_corr_ab = np.corrcoef(preds_a, preds_b)[0, 1]\n",
    "            suc_corr_ab = np.corrcoef(preds_a[non_failed_preds_idx], preds_b[non_failed_preds_idx])[0, 1]\n",
    "            # print(f\"Overall {precision_a} - {precision_b} corr: {ovr_corr_ab:.4f}\")\n",
    "            # print(f\"Success {precision_a} - {precision_b} corr: {suc_corr_ab:.4f}\")\n",
    "\n",
    "            correlation_entry['ovr_corr'] = ovr_corr_ab\n",
    "            correlation_entry['suc_corr'] = suc_corr_ab\n",
    "\n",
    "            # print(json.dumps(correlation_entry, indent=4))\n",
    "\n",
    "            overall_correlation.append(correlation_entry)\n",
    "\n",
    "    overall_performance.append(model_entry)\n",
    "    print()\n",
    "\n",
    "overall_correlation = pd.DataFrame(overall_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_correlation.to_excel('quantization_effects.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ATIS]\n",
      "0.8037 average equivalent prediction atis 4bit vs 8bit\n",
      "0.6680 pearsons r\n",
      "[SNIPS]\n",
      "0.8061 average equivalent prediction snips 4bit vs 8bit\n",
      "0.7977 pearsons r\n",
      "[CLINIC150]\n",
      "0.8938 average equivalent prediction clinic150 4bit vs 8bit\n",
      "0.8903 pearsons r\n",
      "[MASSIVE]\n",
      "0.8001 average equivalent prediction massive 4bit vs 8bit\n",
      "0.8672 pearsons r\n"
     ]
    }
   ],
   "source": [
    "model_type = 'llama-3.1-8b-instruct'\n",
    "model_name = 'Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "for task in tasks:\n",
    "    l4, p4, t4, m4, _ = load_data(task, model_type, model_name, quantization[0])\n",
    "    l8, p8, t8, m8, _ = load_data(task, model_type, model_name, quantization[1])\n",
    "    # lf, pf, tf, mf = load_data(task, model_type, model_name, quantization[2])\n",
    "\n",
    "    compare_predictions(p4, p8, f\"{task} 4bit vs 8bit\")\n",
    "    # compare_predictions(p8, pf, \"8bit vs full\")\n",
    "    # compare_predictions(p4, pf, \"4bit vs full\")\n",
    "\n",
    "\n",
    "assert (l4 == l8).all()  # Sanity check, labels should all be the same\n",
    "# assert (l4 == l8).all() and (l4 == lf).all()  # Sanity check, labels should all be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
